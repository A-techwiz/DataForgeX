# -*- coding: utf-8 -*-
"""Final_Project28.02.23

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K98YTLZtyTc3jecATkZBlhNT8BiZj3zB
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

df=pd.read_excel('/content/Dataset_Churn_Updated_NoisyNull_Values.xlsx')

df.shape

# As can be seen, we have noise in the data in the Country column. 
df.head()

df.dtypes

df.info

df.describe()

# As can be seen, we have missing values in the State column, Churn Reason, and Monthly charges. 
df.isna().sum()

# Checking noise in the data
df['Country']

#Dealing with noise
df.replace(to_replace='abc', value='United States', inplace=True)
df['Country']

df['State']

# Filling missing values in state
df['State'].isnull().sum()

df['State'].fillna('California', inplace=True)

df['State']

df.head()

df['State'].isnull().sum()

#replacing the missing value in the State section
df.loc[2,'State']='California'

df.head()

df.shape

df.dtypes

# organising the columns
df['Churn Reason'].unique()

df['City'].unique()

#dropping columns for predictive analysis
df.drop(['CustomerID','Count', 'Country', 'State', 'Zip Code','City','Churn Label', 'Churn Reason'], axis=1, inplace=True)

df.shape

df.head()

# exloring target variable
df['Churn Value'].value_counts()

# Observations
# As can be seen from the data, we have analysed the target variable. 
# In the target variable, we have the Churn Value which is not balanced. 
# The target variable is distributed into two numbers, 0 and 1.
# 1 means that the customer has churned while no means that the customer did not churn. 
# visualising the target variable
# Observations
sns.countplot(x='Churn Value', data=df)
plt.title('1=Yes 0=No')

# exploring other variables
df.isnull().sum()

missing_values = df["Monthly Charges"].isna()

(missing_values)

df.head()

df.dropna(subset=["Monthly Charges"], inplace=True, axis=0)

df.isnull().sum()

#Visualising missing values
import missingno as msno

msno.matrix(df)

df.dropna(inplace=True)

df.isnull().sum()

# Distribution of the binary variables

fig, axes = plt.subplots(2,3, figsize = (12,8), sharey = True)
plt.suptitle("Distribution of binary variables in the data")

sns.countplot(x='Gender', data=df, ax=axes[0,0])
sns.countplot(x='Senior Citizen', data=df, ax=axes[0,1])
sns.countplot(x='Partner', data=df, ax=axes[0,2])
sns.countplot(x='Dependents', data=df, ax=axes[1,0])
sns.countplot(x='Phone Service', data=df, ax=axes[1,1])
sns.countplot(x='Paperless Billing', data=df, ax=axes[1,2])

# Observations
# Most of the customers in the dataset are not senior citizens
# As indicated in the data card, most of the people do not live with any dependents (Kids, parent, etc)
# Majority of the customers have subscribed to phone service 
# Most of the customers have opted for a paperless billing making it more environment friendly

"""# Observations
Most of the customers in the dataset are not senior citizens
 As indicated in the data card, most of the people do not live with any dependents (Kids, parent, etc)
# Majority of the customers have subscribed to phone service 
# Most of the customers have opted for a paperless billing making it more environment friendly
"""

df.dtypes

#exploring categorical variables
df['Tenure Months']

minimum_value=df['Tenure Months'].min()
(minimum_value)

maximum_value=df['Tenure Months'].max()
(maximum_value)

# This shows the distribution of the tenure of customers
sns.distplot(df['Tenure Months'], kde=False, bins=10, color='blue')
plt.xlabel('Tenure Months')
plt.ylabel('Customers')
plt.title('Distribution of Tenure')
plt.show()

df.shape

# Examining other variables
# Internet services to see the distribution

internet_service=df['Internet Service'].value_counts()
(internet_service)

fig, ax=plt.subplots(figsize=(5,5))
labels=['Fiber optic','DSL', 'No']
plt.pie(internet_service, labels=labels, autopct='%1.1f%%', startangle=90)
plt.title('Internet Service Distribution')
plt.legend(loc='lower right', bbox_to_anchor=(1.75,0.5))
plt.show()

df['Internet Service'].unique()

# Exploring the features of internet services

fig, axes = plt.subplots(2,3, figsize = (12,10), sharey = True)
plt.suptitle('Customer Distribution Across Internet Services')
sns.countplot(x='Online Security', data = df, ax=axes[0,0], order = df['Online Security'].value_counts().index)
sns.countplot(x='Online Backup', data = df, ax=axes[0,1], order = df['Online Backup'].value_counts().index)
sns.countplot(x='Device Protection', data = df, ax=axes[0,2], order = df['Device Protection'].value_counts().index)
sns.countplot(x='Tech Support', data = df, ax=axes[1,0], order = df['Tech Support'].value_counts().index)
sns.countplot(x='Streaming TV', data = df, ax=axes[1,1], order = df['Streaming TV'].value_counts().index)
sns.countplot(x='Streaming Movies', data = df, ax=axes[1,2], order = df['Streaming Movies'].value_counts().index)

# Examining the contract variable to see its distribution
contract=df['Contract'].value_counts()
(contract)

# Examining the duraiton of the contract
sns.countplot(x = 'Contract', data = df)
plt.title('Customers by Contract Type')

# Examining the payment method to see its distribution
payment_method=df['Payment Method'].value_counts()
(payment_method)

# Exploring the payment method

plt.figure(figsize = (10,6))
sns.countplot(x = 'Payment Method', data = df, order = df['Payment Method'].value_counts().index)
plt.title('Contract Type Of Customers')

# Examining the impact of the variables on churn rate
df.head()

df[['Gender','Churn Value']].groupby(['Gender']).mean()

df[['Senior Citizen','Churn Value']].groupby(['Senior Citizen']).mean()

df[['Partner','Churn Value']].groupby(['Partner']).mean()

df[['Dependents','Churn Value']].groupby(['Dependents']).mean()

# Examining categorical variables
# Internet services
sns.countplot(x = "Internet Service", data = df,order = df['Internet Service'].value_counts().index)

# Fiber optic subscribers have a higher churn rate
df[['Internet Service', 'Churn Value']].groupby('Internet Service').mean().sort_values(by= 'Churn Value', ascending = False)

# Fiber optic has the highest monthly rate
df[['Monthly Charges', 'Internet Service']].groupby('Internet Service').mean().sort_values(by = 'Monthly Charges', ascending= False)

# Observations
# From the above information, we can conclude that users who have subscribed to fiber optic tend to churn more.

# Exploring the impact of payment method on churn value. 
df[['Payment Method', 'Churn Value']].groupby('Payment Method').mean().sort_values(by = 'Churn Value', ascending=False)

plt.figure(figsize = (10,6))
sns.countplot(x = 'Payment Method', data = df, order = df['Payment Method'].value_counts().index)
plt.title('Contract Type Of Customers')

# Observations
# Customers who pay via electronic check are more likely to churn.
# The payment method is widely used by the users.

# Exploring Monthly Charges
sns.displot(data=df, x='Monthly Charges',hue='Churn Value',kind = "kde")
plt.title('Monthly Charges and Churn Value', fontsize=14)
plt.text(x=0, y=0, s="Churn Value 1=Yes 0=No", fontsize=12, fontweight='light')

# Observations
# As the monthly charges go up, the churn rate also increases

# Explporing Tenure
sns.displot(data =df, x = "Tenure Months", hue = "Churn Value", kind = "kde")
plt.title('Tenure and Churn Value Comparison', fontsize=14)

# observations
# Customers with a lower tenure tend to churn more

# Feature selection
df.columns

# Further trimming data
df.drop(['Lat Long', 'Latitude', 'Longitude','Gender','Total Charges','Churn Score','CLTV'], axis=1, inplace=True)

df.shape

# Categorical columns
catcol=df.select_dtypes(include=['object']).columns.tolist()
print(catcol)
len(catcol)

for col in catcol:
  print(col)

# One hot encoding vs count encoding
# with one hot encoding, we might face the problem of curse of dimensionality
# Count encoding has the advantage of preserving the information contained in categorical variables and reducing the dimensionality of the data.

encoded_df=pd.get_dummies(df, columns=catcol, drop_first=True)

# For encoding ask maam.
# Drop City or No?
encoded_df.head()

encoded_df.info()

# uint8 is a numeric data type in Python that represents unsigned 8-bit integers. 
# The "u" in uint8 stands for "unsigned", meaning that the integer values cannot be negative.

df.dtypes

# Model building and model selection
from sklearn.model_selection import train_test_split

# drop churn value in the test train split and try to build the model. 
# Notice here inplaxe is not given. So it means it will create a new dataset without churn value
# X is explanatory variable
X = encoded_df.drop('Churn Value', axis = 1)
Y= encoded_df['Churn Value']

encoded_df.shape

# Separating the dataset into train and test set
X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.2,random_state = 1)

# Checking the shape of the testing and training data
X_train.shape

X_test.shape

Y_train.shape

Y_test.shape

# Logistic Regression Model
# Ask about maximum iterations
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

#The max_iteration hyperparameter specifies the maximum number of iterations allowed 
#for the solver to converge to the optimal solution. 
#If the solver does not converge within the specified number of iterations
#it will stop and return the best solution found so far, which may not necessarily be the optimal solution.

model=LogisticRegression (max_iter=700)

print(model)

model.fit(X_train, Y_train)

# Accuracy score on training data
train_pred = model.predict(X_train)
acc_train = accuracy_score(train_pred, Y_train)
print("Accuracy score on trianing data:",acc_train)

# Accuracy score on test data
test_pred= model.predict(X_test)
acc_test= accuracy_score(test_pred,Y_test )
print('Accuracy score on test data:', acc_test)

#Checking precision and recall scores
from sklearn.metrics import recall_score
from sklearn.metrics import precision_score
from sklearn import metrics

log_recall = recall_score(Y_test,test_pred)
log_precision = precision_score(Y_test,test_pred)
print("LogisticRegression model's metrics:\n")
print("Accuracy on Training Data:", round(acc_train, 3))
print("Accuracy on Test Data:", round(acc_test,3))
print("Recall Score:", round(log_recall,3))
print("Precision Score:", round(log_precision,3))

# Building a confusion matrix

confusion_matrix = metrics.confusion_matrix(Y_test, test_pred)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix,display_labels = ['Negative', 'Positive'])
cm_display.plot()
plt.title('Confusion Matrix: LogisticRegression')
plt.show()

